
---

# ğŸ“˜ **README â€” MS Support Chatbot (RAG + Memory)**

## ğŸ§  Overview

This project is a **Microsoft Support Chatbot** powered by:

* **RAG (Retrieval-Augmented Generation)**
* **LLaMA/Gemma LLM running locally via llama-cpp-python**
* **ChromaDB** vector database
* **Persistent chat memory (SQLite-based)**
* **Clean modern frontend built in HTML/CSS/JS**

The chatbot can:

âœ” Answer user questions based on your custom documents
âœ” Remember previous conversation history
âœ” Retrieve relevant chunks from your document database
âœ” Respond like a Microsoft support agent
âœ” Run fully locally â€” *no cloud required*

---

## ğŸš€ Project Structure

```
project/
â”‚
â”œâ”€â”€ RAg.py                 # FastAPI backend + RAG + memory
â”œâ”€â”€ chat_memory.db         # Persistent conversation memory (auto-generated)
â”œâ”€â”€ chroma_db/             # Vector database folder
â”œâ”€â”€ model/                 # Local LLaMA/Gemma GGUF model file
â”œâ”€â”€ data/                  # Your documents for RAG indexing
â”‚
â””â”€â”€ frontend/
    â””â”€â”€ index.html         # Chat UI 
```

---

## ğŸ§© Requirements

Install required packages:

```bash
pip install -r requirements.txt
```

Key dependencies include:

* FastAPI
* Uvicorn
* ChromaDB
* llama-cpp-python (GPU/CPU)
* sqlite3 (built-in)
* Python 3.10+

---

## ğŸ“¥ Download Resources

### ğŸ”— **1. Model File (.gguf â€” LLaMA/Gemma)**

â¡ï¸ *Paste your model download link here*

```
MODEL DOWNLOAD LINK:
https://www.kaggle.com/models/omarabdulqadir1/llm
```

Place the downloaded `.gguf` file inside:

```
/model
```

---

### ğŸ”— **2. Chroma Vector Database (Documents Index)**

â¡ï¸ *Paste your vector DB link here*

```
VECTOR DATABASE DOWNLOAD LINK:
https://www.kaggle.com/code/omarehab9/rag-application-demo
```

Extract and place it in:

```
/chroma_db
```

---

## ğŸƒâ€â™‚ï¸ Running the Project

### 1ï¸âƒ£ Start the Backend (FastAPI)

```bash
uvicorn RAg:app --reload --port 8000
```

Backend URL:

```
http://localhost:8000
```

---

### 2ï¸âƒ£ Open the Frontend

Option A â€” Open directly

```
frontend/index.html
```

Option B â€” Serve it (recommended):

```bash
cd frontend
python -m http.server 5500
```

Then visit:

```
http://localhost:5500
```

---

## ğŸ§  How Chat Memory Works

The project uses **SQLite-based persistent memory**:

* Each user session is tracked using a `session_id`
* Every message is stored in `chat_memory.db`
* Memory is appended and loaded at every request
* Helps the model maintain context across multiple chat turns

---

## ğŸ— How RAG Works

1. User sends a message
2. Memory is loaded
3. Query is passed into the RAG chain
4. ChromaDB retrieves relevant document chunks
5. LLM uses:

   * conversation history
   * retrieved knowledge
6. Generates a contextual, helpful answer

---

## ğŸ–¥ Frontend Features

* Animated typing indicator
* Modern UI with gradients and shadow effects
* Auto-scroll
* Error handling
* Smooth animations
* Ready to deploy as a static site

---

## ğŸ”® Future Improvements

* Add user authentication
* Add conversation summarization memory
* Add file upload for new documents
* Add Docker support
* Add dark/light theme toggle

---

## ğŸ“„ License

This project is private unless you choose to make it open source.
You may freely modify and distribute your customized version.

---
